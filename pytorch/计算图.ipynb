{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 计算图\n",
    "计算图是一个有向无环图，它是一种特殊的数据结构，它由节点和边组成，节点表示数据，边表示计算关系。\n",
    "使用torch对算法公式进行实现时，会将算法公式转换为计算图\n",
    "计算图是承载公式算法的数据结构\n",
    "\n",
    "## 计算图的组成\n",
    "计算图由节点和边组成，节点表示数据，边表示计算关系\n",
    "使用张量（tensor）表示数据\n",
    "使用函数（function）表示计算关系\n",
    "\n",
    "### 梯度\n",
    "梯度是一个向量，表示某个函数在该点处的方向导数沿着该方向取得最大值，梯度的方向是函数在该点处的方向导数沿着该方向取得最大值，梯度的模是函数在该点处的方向导数沿着该方向取得最大值\n",
    "在深度学习中，梯度是指函数关于变量的偏导数。\n",
    "对于PyTorch中的张量（tensor），我们可以理解为张量的梯度是一个与张量形状相同的张量，用于表示该张量对于某个函数的梯度值。\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 自动求导\n",
    "\n",
    "PyTorch的自动求导（Autograd）是PyTorch中的一个重要功能，用于自动计算张量的梯度。\n",
    "## 计算梯度的作用：\n",
    "计算张量的梯度在机器学习中非常重要。梯度可以告诉我们目标函数（损失函数）相对于张量的变化率，即在当前点上函数变化最快的方向。\n",
    "通过计算梯度，我们可以进行优化算法，寻找使目标函数（损失函数）最小化或最大化的阐述取值。\n",
    "\n",
    "在机器学习中，我们通常使用梯度下降等优化算法来更新模型参数，使模型能够逐步逼近最优解。通过计算张量的梯度，我们可以确定在哪个方向上调整张量的取值，以使目标函数不断接近最优解。\n",
    "\n",
    "```\n",
    "The gradient of a tensor represents the rate of change of a function with respect to that tensor. It provides valuable information about how the function behaves in the vicinity of the tensor's current value.\n",
    "\n",
    "In machine learning, the gradient of a tensor is essential for optimization algorithms such as gradient descent. By calculating the gradient, we can determine the direction in which the tensor should be updated to minimize or maximize a given objective function. The gradient guides the parameter updates, allowing the model to iteratively improve its performance.\n",
    "\n",
    "The gradient is particularly important in training neural networks. It is used to update the weights and biases of the network during backpropagation, enabling the network to learn from the training data and adjust its parameters to minimize the loss function. By iteratively updating the parameters based on the gradient, the network gradually converges towards a better solution.\n",
    "```\n",
    "## torch.autograd\n",
    "autograd能够根据用户定义的计算图自动计算梯度。\n",
    "\n",
    "当一个张量的 requires_grad 属性被设置为True时，PyTorch会跟踪该张量的计算过程，并在计算图中构建梯度图。然后，可以通过调用 .backward() 方法自动计算梯度，并将梯度值保存在每个张量的 .grad 属性中。\n",
    "\n",
    "利用Autograd，我们可以方便地进行梯度下降优化、反向传播和参数更新。它使得构建和训练复杂的神经网络变得更加简单和高效。同时，Autograd还支持动态计算图的设计思想，使得PyTorch具有灵活性和动态性，能够处理动态的计算流程和复杂的模型结构。\n",
    "\n",
    "总之，Autograd的作用是自动计算张量的梯度，为深度学习中的优化和反向传播提供了便利和效率。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### torch.autograd.backward\n",
    "功能：自动求取梯度 （求导）\n",
    "```python\n",
    "torch.autograd.backward(tensors = out_t,\n",
    "                        grad_tensors=None,\n",
    "                        retain_graph=None,\n",
    "                        create_graph=False)\n",
    "```\n",
    "- tensors 被求导的张量 如 loss\n",
    "- grad_tensors 多梯度权重\n",
    "- retain_graph 是否保存计算图\n",
    "- create_graph 创建导数计算图 用于高阶求导\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "out_t = torch.tensor([11])\n",
    "torch.autograd.backward(tensors = out_t,\n",
    "                        grad_tensors=None,\n",
    "                        retain_graph=None,\n",
    "                        create_graph=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
